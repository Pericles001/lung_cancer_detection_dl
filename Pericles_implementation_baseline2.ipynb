{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9801516,"sourceType":"datasetVersion","datasetId":6007220}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iabh1shekbasu/LungCancerDetectionEnsemble/blob/main/Probability_Extraction_and_Analysis.ipynb\n\n)\n","metadata":{"id":"lfzxkoFg4ePS"}},{"cell_type":"markdown","source":"## Connecting Google Drive","metadata":{"id":"rAhpqH6d3j81"}},{"cell_type":"code","source":"!pip cache purge\n# !!pip install numpy==1.22.4 pandas==1.5.3 scipy==1.8.1\n# !pip cache purge\n# !pip install scipy==1.7.3\n# !pip install --force-reinstall --no-deps scipy==1.14.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:33:37.680124Z","iopub.execute_input":"2024-11-04T11:33:37.680514Z","iopub.status.idle":"2024-11-04T11:33:39.123044Z","shell.execute_reply.started":"2024-11-04T11:33:37.680478Z","shell.execute_reply":"2024-11-04T11:33:39.121917Z"}},"outputs":[{"name":"stdout","text":"Files removed: 30\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"!pip install --force-reinstall --no-cache-dir scipy==1.14.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:33:52.495319Z","iopub.execute_input":"2024-11-04T11:33:52.495715Z","iopub.status.idle":"2024-11-04T11:34:08.973735Z","shell.execute_reply.started":"2024-11-04T11:33:52.495677Z","shell.execute_reply":"2024-11-04T11:34:08.972611Z"}},"outputs":[{"name":"stdout","text":"Collecting scipy==1.14.1\n  Downloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting numpy<2.3,>=1.23.5 (from scipy==1.14.1)\n  Downloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading numpy-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for scipy: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/scipy-1.14.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: numpy, scipy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.22.4\n    Uninstalling numpy-1.22.4:\n      Successfully uninstalled numpy-1.22.4\n  Attempting uninstall: scipy\n\u001b[33m    WARNING: No metadata found in /opt/conda/lib/python3.10/site-packages\u001b[0m\u001b[33m\n\u001b[0m    Found existing installation: scipy 1.14.1\n\u001b[31mERROR: Cannot uninstall scipy 1.14.1, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps scipy==1.14.1'.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip uninstall scikit-plot -y\n!pip install scikit-plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:13:14.366869Z","iopub.execute_input":"2024-11-04T11:13:14.367170Z","iopub.status.idle":"2024-11-04T11:13:18.308222Z","shell.execute_reply.started":"2024-11-04T11:13:14.367138Z","shell.execute_reply":"2024-11-04T11:13:18.306893Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: scikit-plot 0.3.7\nUninstalling scikit-plot-0.3.7:\n  Successfully uninstalled scikit-plot-0.3.7\nCollecting scikit-plot\n  Downloading scikit_plot-0.3.7-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: matplotlib>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (3.7.5)\nRequirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (1.2.2)\nRequirement already satisfied: scipy>=0.9 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (1.14.1)\nRequirement already satisfied: joblib>=0.10 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (1.4.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.22.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.9.0.post0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->scikit-plot) (3.5.0)\n\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/scipy-1.14.1.dist-info/METADATA'\n\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:17:43.049714Z","iopub.execute_input":"2024-11-04T11:17:43.050567Z","iopub.status.idle":"2024-11-04T11:17:44.039277Z","shell.execute_reply.started":"2024-11-04T11:17:43.050528Z","shell.execute_reply":"2024-11-04T11:17:44.038021Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:08:35.570108Z","iopub.execute_input":"2024-11-04T11:08:35.570479Z","iopub.status.idle":"2024-11-04T11:08:47.220544Z","shell.execute_reply.started":"2024-11-04T11:08:35.570429Z","shell.execute_reply":"2024-11-04T11:08:47.219542Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (1.14.1)\nRequirement already satisfied: numpy<2.3,>=1.23.5 in /opt/conda/lib/python3.10/site-packages (from scipy) (1.26.4)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# !pip uninstall scikit-plot -y\n!pip install scikit-plot","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:05:48.304722Z","iopub.execute_input":"2024-11-04T11:05:48.305374Z","iopub.status.idle":"2024-11-04T11:06:03.359337Z","shell.execute_reply.started":"2024-11-04T11:05:48.305332Z","shell.execute_reply":"2024-11-04T11:06:03.358189Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: scikit-plot in /opt/conda/lib/python3.10/site-packages (0.3.7)\nRequirement already satisfied: matplotlib>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (3.7.5)\nRequirement already satisfied: scikit-learn>=0.18 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (1.2.2)\nRequirement already satisfied: scipy>=0.9 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (1.14.1)\nRequirement already satisfied: joblib>=0.10 in /opt/conda/lib/python3.10/site-packages (from scikit-plot) (1.4.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.2.1)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (4.53.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (21.3)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (10.3.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.9.0.post0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.18->scikit-plot) (3.5.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nimport math,os,argparse\nfrom scikitplot.estimators import plot_feature_importances\nfrom scikitplot.metrics import plot_confusion_matrix, plot_roc\nfrom sklearn.metrics import roc_curve,auc\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import label_binarize","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-04T11:09:24.844253Z","iopub.execute_input":"2024-11-04T11:09:24.844619Z","iopub.status.idle":"2024-11-04T11:09:25.030680Z","shell.execute_reply.started":"2024-11-04T11:09:24.844586Z","shell.execute_reply":"2024-11-04T11:09:25.029417Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mestimators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_feature_importances\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix, plot_roc\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m roc_curve,auc\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scikitplot/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import, division, print_function, unicode_literals\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics, cluster, decomposition, estimators\n\u001b[1;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0.3.7\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classifier_factory\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/scikitplot/metrics.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcalibration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calibration_curve\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m interp\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binary_ks_curve, validate_labels\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikitplot\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhelpers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cumulative_gain_curve\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'interp' from 'scipy' (/opt/conda/lib/python3.10/site-packages/scipy/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'interp' from 'scipy' (/opt/conda/lib/python3.10/site-packages/scipy/__init__.py)","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# !mkdir /content/drive","metadata":{"id":"jHg6TA_x_GBG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from google.colab import drive\n\n# drive.mount('/content/drive')\n# https://drive.google.com/drive/folders/1pk4XWbn5PVPS4OQmj3GhBHq5fd0QAYZk?usp=drive_link","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5VJCY8ut_j8I","outputId":"5e15b818-52f9-493d-c953-a0ab78a8d5c3","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !google-drive-ocamlfuse\n\n!ls '/kaggle/input/ensemble-learning-on-lidc-dataset/data'","metadata":{"id":"dgsT6mC3tQF-","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e6a8de35-2e21-4382-a15d-020d181f70e2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !xdg-settings set default-web-browser w3m.desktop # to set default browser\n\n# %cd /content\n\n# !mkdir drive\n\n# %cd drive\n\n# !mkdir MyDrive\n\n# %cd ..\n\n# %cd ..\n\n# !google-drive-ocamlfuse /content/drive/MyDrive","metadata":{"id":"MBozFjkZgUG2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Libraries\n\n\n\nThis section imports necessary libraries required for the entire notebook. It includes deep learning libraries such as PyTorch, data manipulation libraries like NumPy, and visualization libraries such as matplotlib.","metadata":{"id":"20b1c6af"}},{"cell_type":"code","source":"import time\n\nimport os\n\nimport copy\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\n\nimport torch.nn as nn\n\nimport torch.optim as optim\n\nfrom torch.optim import lr_scheduler\n\nimport torchvision\n\nfrom torchvision import datasets, models, transforms\n\nfrom torch.utils.data import WeightedRandomSampler","metadata":{"id":"5fyYNJJ-wXVL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Preprocessing\n\n\n\nThis section defines the transformations to be applied to the input data for training and evaluation purposes. It includes normalization, resizing, and augmentation strategies.","metadata":{"id":"e2ea2095"}},{"cell_type":"code","source":"mean = np.array([0.485, 0.456, 0.406])\n\nstd = np.array([0.229, 0.224, 0.225])","metadata":{"id":"phwkI5VowXcW","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_transforms = {\n\n    'train': transforms.Compose([\n\n        transforms.Resize((256,256)),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize(mean, std),\n\n    ]),\n\n    'val': transforms.Compose([\n\n        transforms.Resize((256,256)),\n\n        transforms.ToTensor(),\n\n    ]),\n\n    'test': transforms.Compose([\n\n       transforms.Resize((256,256)),\n\n        transforms.ToTensor(),\n\n    ]),\n\n}","metadata":{"id":"9oGdFrDawXeu","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Directory Setup\n\n\n\nThis sets the path to the directory where the dataset is stored. It's essential for the notebook to access the training and testing data.","metadata":{"id":"6c021840"}},{"cell_type":"code","source":"# data_dir = \"/content/drive/MyDrive/Ensemble Learning on LIDC Dataset/data\"  # Set the directory for the data\n\n# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\n#                                           data_transforms[x])\n\n#                   for x in [ 'test', 'train', 'val']}\n\n# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n\n#                                              shuffle=True, num_workers=2)\n\n#               for x in ['train', 'val','test']}\n\n# dataset_sizes = {x: len(image_datasets[x]) for x in ['test', 'train', 'val']}\n\n# class_names = image_datasets['train'].classes\n\n# num_classes = len(class_names)\n\n# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# print(class_names)\n\n# Set the path to the data directory\n\ndata_dir = \"/kaggle/input/ensemble-learning-on-lidc-dataset/data\"\n\n\n\n# Verify the contents of the data directory\n\nprint(\"Contents of data directory:\")\n\n!ls \"{data_dir}\"\n\n\n\n# Load the datasets\n\ndata_transforms = {\n\n    'train': transforms.Compose([\n\n        transforms.Resize((256, 256)),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize(mean, std),\n\n    ]),\n\n    'val': transforms.Compose([\n\n        transforms.Resize((256, 256)),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize(mean, std),\n\n    ]),\n\n    'test': transforms.Compose([\n\n        transforms.Resize((256, 256)),\n\n        transforms.ToTensor(),\n\n        transforms.Normalize(mean, std),\n\n    ]),\n\n}\n\n\n\n# Define the datasets and dataloaders\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n\n                  for x in ['train', 'val', 'test']}\n\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n\n                                             shuffle=True, num_workers=2)\n\n               for x in ['train', 'val', 'test']}\n\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n\nclass_names = image_datasets['train'].classes\n\nnum_classes = len(class_names)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n\n\n# Display the class names\n\nprint(\"Classes:\", class_names)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0TxSDbhXwXhD","outputId":"e8293fb3-6a4c-4be8-88d5-237411dcc66b","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization Function\n\n\n\nHere we define a function to visualize images in the dataset. It will help in understanding the data and debugging the data loaders.","metadata":{"id":"97b79c30"}},{"cell_type":"code","source":"def imshow(inp, title):\n\n    \"\"\"Imshow for Tensor.\"\"\"\n\n    inp = inp.numpy().transpose((1, 2, 0))\n\n    inp = std * inp + mean\n\n    inp = np.clip(inp, 0, 1)\n\n    plt.imshow(inp)\n\n    plt.title(title)\n\n    plt.show()","metadata":{"id":"t957VPeRwXjx","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a batch of testing data\n\ninputs, classes = next(iter(dataloaders['test']))\n\n# Make a grid from batch\n\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","metadata":{"id":"S3t4wxmKwXmK","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"a483b3e9-79ae-438d-91b5-fd30a88d1688","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot(val_loss,train_loss,typ):\n\n    plt.title(\"{} after epoch: {}\".format(typ,len(train_loss)))\n\n    plt.xlabel(\"Epoch\")\n\n    plt.ylabel(typ)\n\n    plt.plot(list(range(len(train_loss))),train_loss,color=\"r\",label=\"Train \"+typ)\n\n    plt.plot(list(range(len(val_loss))),val_loss,color=\"b\",label=\"Validation \"+typ)\n\n    plt.legend()\n\n    plt.savefig(os.path.join(data_dir,typ+\".png\"))\n\n    plt.close()","metadata":{"id":"wCNW2ZBbwXod","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss_gph=[]\n\ntrain_loss_gph=[]\n\nval_acc_gph=[]\n\ntrain_acc_gph=[]","metadata":{"id":"DZ4sbUwCwXq_","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training Function\n\n\n\nThis function encapsulates the model training logic. It takes a model, criterion for loss calculation, optimizer for backpropagation, and a scheduler for learning rate adjustment as inputs and conducts the training process.","metadata":{"id":"fc576835"}},{"cell_type":"code","source":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25,model_name = \"kaggle\"):\n\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n\n        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n\n        for phase in ['train', 'val']:\n\n            if phase == 'train':\n\n                model.train()  # Set model to training mode\n\n            else:\n\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n\n            running_corrects = 0\n\n            # Iterate over data.\n\n            for inputs, labels in dataloaders[phase]:\n\n                inputs = inputs.to(device)\n\n                labels = labels.to(device)\n\n                # forward\n\n                # track history if only in train\n\n                with torch.set_grad_enabled(phase == 'train'):\n\n                    outputs = model(inputs)\n\n                    _, preds = torch.max(outputs, 1) #was (outputs,1) for non-inception and (outputs.data,1) for inception\n\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n\n                    if phase == 'train':\n\n                        optimizer.zero_grad()\n\n                        loss.backward()\n\n                        optimizer.step()\n\n                # statistics\n\n                running_loss += loss.item() * inputs.size(0)\n\n                running_corrects += torch.sum(preds == labels.data)\n\n            if phase == 'train':\n\n                scheduler.step()\n\n            epoch_loss = running_loss / dataset_sizes[phase]\n\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            if phase == 'train':\n\n              train_loss_gph.append(epoch_loss)\n\n              train_acc_gph.append(epoch_acc)\n\n            if phase == 'val':\n\n              val_loss_gph.append(epoch_loss)\n\n              val_acc_gph.append(epoch_acc)\n\n           # plot(val_loss_gph,train_loss_gph, \"Loss\")\n\n          #  plot(val_acc_gph,train_acc_gph, \"Accuracy\")\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n\n            if phase == 'val' and epoch_acc >= best_acc:\n\n                best_acc = epoch_acc\n\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n                torch.save(model, data_dir+\"/\"+model_name+\".h5\")\n\n                print('==>Model Saved')\n\n        print()\n\n    time_elapsed = time.time() - since\n\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n\n        time_elapsed // 60, time_elapsed % 60))\n\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n\n    model.load_state_dict(best_model_wts)\n\n    return model\n\n\n# =====================\n# import os\n\n# import time\n\n# import copy\n\n# import torch\n\n\n\n# def train_model(model, criterion, optimizer, scheduler, num_epochs=25, model_name=\"kaggle\"):\n\n#     since = time.time()\n\n#     best_model_wts = copy.deepcopy(model.state_dict())\n\n#     best_acc = 0.0\n\n\n\n#     # Ensure save directory exists\n\n#     os.makedirs(save_dir, exist_ok=True)\n\n\n\n#     for epoch in range(num_epochs):\n\n#         print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n\n#         print('-' * 10)\n\n\n\n#         # Each epoch has a training and validation phase\n\n#         for phase in ['train', 'val']:\n\n#             if phase == 'train':\n\n#                 model.train()  # Set model to training mode\n\n#             else:\n\n#                 model.eval()   # Set model to evaluate mode\n\n\n\n#             running_loss = 0.0\n\n#             running_corrects = 0\n\n\n\n#             # Iterate over data.\n\n#             for inputs, labels in dataloaders[phase]:\n\n#                 inputs = inputs.to(device)\n\n#                 labels = labels.to(device)\n\n\n\n#                 # Forward pass\n\n#                 with torch.set_grad_enabled(phase == 'train'):\n\n#                     outputs = model(inputs)\n\n#                     _, preds = torch.max(outputs, 1)\n\n#                     loss = criterion(outputs, labels)\n\n\n\n#                     # Backward + optimize only if in training phase\n\n#                     if phase == 'train':\n\n#                         optimizer.zero_grad()\n\n#                         loss.backward()\n\n#                         optimizer.step()\n\n\n\n#                 # Statistics\n\n#                 running_loss += loss.item() * inputs.size(0)\n\n#                 running_corrects += torch.sum(preds == labels.data)\n\n\n\n#             if phase == 'train':\n\n#                 scheduler.step()\n\n\n\n#             epoch_loss = running_loss / dataset_sizes[phase]\n\n#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n\n\n#             # Append losses and accuracy for tracking\n\n#             if phase == 'train':\n\n#                 train_loss_gph.append(epoch_loss)\n\n#                 train_acc_gph.append(epoch_acc)\n\n#             elif phase == 'val':\n\n#                 val_loss_gph.append(epoch_loss)\n\n#                 val_acc_gph.append(epoch_acc)\n\n\n\n#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n\n\n#             # Deep copy the model if it has the best accuracy on validation set\n\n#             if phase == 'val' and epoch_acc >= best_acc:\n\n#                 best_acc = epoch_acc\n\n#                 best_model_wts = copy.deepcopy(model.state_dict())\n\n#                 torch.save(model, os.path.join(save_dir, model_name + \".pth\"))\n\n#                 print('==>Model Saved')\n\n\n\n#         print()\n\n\n\n#     time_elapsed = time.time() - since\n\n#     print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n\n#     print('Best val Acc: {:4f}'.format(best_acc))\n\n\n\n#     # Load best model weights\n\n#     model.load_state_dict(best_model_wts)\n\n#     return model\n","metadata":{"id":"4MUg_T9FwXty","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# ResNet 152","metadata":{"id":"7wZ0_Qfo5f8q"}},{"cell_type":"markdown","source":"## Model Definition and Training\n\n\n\nThis section covers the instantiation of the ResNet152 model and its subsequent training with the dataset.","metadata":{"id":"c9302949"}},{"cell_type":"code","source":"model = models.resnet152(pretrained = True)\n\n#num_ftrs = model.classifier[0].in_features\n\nnum_ftrs = model.fc.in_features  ##for googlenet, resnet18\n\n#num_ftrs = model.classifier.in_features  ## for densenet169\n\nprint(\"Number of features: \"+str(num_ftrs))\n\n#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n\nmodel.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n\n#model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n\n# Observe that all parameters are being optimized\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n\n# Decay LR by a factor of 0.1 every 7 epochs\n\n# Learning rate scheduling should be applied after optimizer’s update\n\n# e.g., you should write your code this way:\n\n# for epoch in range(100):\n\n#     train(...)\n\n#     validate(...)\n\n#     scheduler.step()\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"resnet152\")","metadata":{"id":"pSghfYK5wXwK","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3a22ec82-afd6-42c3-b3ce-51350c06fbc1","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Getting Proba distribution\n\nprint(\"\\nGetting the Probability Distribution\")\n\ntrainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n\ntestloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n\nmodel=model.eval()\n\nimport csv\n\nimport numpy as np  # Importing NumPy for numerical operations","metadata":{"id":"vGn8_Nw1wXy6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"379b5c39-ec1b-46f0-a36e-bea1d4af2775","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f = open(data_dir+\"/resnet152_train.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(trainloader),num_classes))\n\n      for i,data in enumerate(trainloader):\n\n          images, labels = data\n\n          sample_fname, _ = trainloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Train Accuracy = \",100*correct/total)\n\nfor i in range(len(trainloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(trainloader):\n\n  _, labels = data\n\n  sample_fname, _ = trainloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n# ====================\n\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # Open file for writing predicted probabilities\n\n# f = open(save_dir + \"/resnet152_train.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []  # To store filenames\n\n# correct = 0  # To count correct predictions\n\n# total = 0    # To count total samples\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(trainloader), num_classes))  # Array to store probabilities\n\n\n\n#     # Loop through training data\n\n#     for i, data in enumerate(trainloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print training accuracy\n\n# print(\"Train Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(trainloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/train_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through training data to get labels\n\n# for i, data in enumerate(trainloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"vc-8RBcowX1r","colab":{"base_uri":"https://localhost:8080/"},"outputId":"76e4427c-4082-49fc-8c24-c250bd6ce965","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Probabilities\n\nf = open(data_dir+\"/resnet152_test.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(testloader),num_classes))\n\n      for i,data in enumerate(testloader):\n\n          images, labels = data\n\n          sample_fname, _ = testloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Test Accuracy = \",100*correct/total)\n\nfor i in range(len(testloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(testloader):\n\n  _, labels = data\n\n  sample_fname, _ = testloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n#=====================\n\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # Test Probabilities\n\n# f = open(save_dir + \"/resnet152_test.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(testloader), num_classes))\n\n\n\n#     # Loop through test data\n\n#     for i, data in enumerate(testloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print test accuracy\n\n# print(\"Test Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(testloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/test_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through test data to get labels\n\n# for i, data in enumerate(testloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"n1DjIkdPwX4O","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0db763c7-aba7-4ede-9013-4efd36ee294e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inception V3","metadata":{"id":"E5Sz8Gm3K7oO"}},{"cell_type":"markdown","source":"## Inception V3 Model\n\n\n\nFollowing the pattern of the previous section, this part focuses on the Inception V3 model, its setup, and training.","metadata":{"id":"fb03cdc6"}},{"cell_type":"code","source":"# model = models.inception_v3(pretrained = True)\n\n# model.aux_logits = False\n\n# # Handle the auxilary net\n\n# num_ftrs = model.AuxLogits.fc.in_features\n\n# model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n\n# # Handle the primary net\n\n# num_ftrs = model.fc.in_features\n\n# model.fc = nn.Linear(num_ftrs,num_classes)\n\n# print(\"Number of features: \"+str(num_ftrs))\n\n# #model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n\n# model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n\n# #model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n\n# model = model.to(device)\n\n# criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n\n# # Observe that all parameters are being optimized\n\n# optimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# # StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n\n# # Decay LR by a factor of 0.1 every 7 epochs\n\n# # Learning rate scheduling should be applied after optimizer’s update\n\n# # e.g., you should write your code this way:\n\n# # for epoch in range(100):\n\n# #     train(...)\n\n# #     validate(...)\n\n# #     scheduler.step()\n\n# step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n\n# model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"inception_v3\")\n\n\n\nfrom torchvision import transforms\n\n\n\ndata_transforms = {\n\n    'train': transforms.Compose([\n\n        transforms.Resize((299, 299)),  # Resize images to 299x299 for Inception v3\n\n        transforms.ToTensor(),\n\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    ]),\n\n    'val': transforms.Compose([\n\n        transforms.Resize((299, 299)),  # Resize images to 299x299 for Inception v3\n\n        transforms.ToTensor(),\n\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    ]),\n\n    'test': transforms.Compose([\n\n        transforms.Resize((299, 299)),  # Resize images to 299x299 for Inception v3\n\n        transforms.ToTensor(),\n\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\n    ]),\n\n}\n","metadata":{"id":"Qs1MXn4oL4oO","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\n\nimport numpy as np\n\n# Getting Proba distribution\n\nprint(\"\\nGetting the Probability Distribution\")\n\ntrainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n\ntestloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n\nmodel=model.eval()","metadata":{"id":"dmh_rbxbMVQy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9e500838-d7e0-44a8-a840-61612140f165","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f = open(data_dir+\"/inception_v3_train.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(trainloader),num_classes))\n\n      for i,data in enumerate(trainloader):\n\n          images, labels = data\n\n          sample_fname, _ = trainloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Train Accuracy = \",100*correct/total)\n\nfor i in range(len(trainloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(trainloader):\n\n  _, labels = data\n\n  sample_fname, _ = trainloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n\n# ===============\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # Inception_v3 Train Probabilities\n\n# f = open(save_dir + \"/inception_v3_train.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(trainloader), num_classes))\n\n\n\n#     # Loop through training data\n\n#     for i, data in enumerate(trainloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print train accuracy\n\n# print(\"Train Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(trainloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/train_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through training data to get labels\n\n# for i, data in enumerate(trainloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"PyQeUPZyMYY0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8ad8564a-53b5-469d-9375-29bd3a6a9d90","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Probabilities\n\nf = open(data_dir+\"/inception_v3_test.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(testloader),num_classes))\n\n      for i,data in enumerate(testloader):\n\n          images, labels = data\n\n          sample_fname, _ = testloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Test Accuracy = \",100*correct/total)\n\nfor i in range(len(testloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(testloader):\n\n  _, labels = data\n\n  sample_fname, _ = testloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n# ====================\n\n\n\n\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # Inception_v3 Test Probabilities\n\n# f = open(save_dir + \"/inception_v3_test.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(testloader), num_classes))\n\n\n\n#     # Loop through test data\n\n#     for i, data in enumerate(testloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print test accuracy\n\n# print(\"Test Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(testloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/test_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through test data to get labels\n\n# for i, data in enumerate(testloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"JV1bWRWrMZRD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"90464e65-a538-4a43-92c4-9c72c2e56738","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Densenet 169","metadata":{"id":"xpFGud4Y5mz3"}},{"cell_type":"markdown","source":"\n\n\n\n## Densenet 169  Model\n\n\n\nFollowing the pattern of the previous section, this part focuses on the Densenet 169 model, its setup, and training.","metadata":{"id":"IeHrwVuZTZ4c"}},{"cell_type":"code","source":"model = models.densenet169(pretrained = True)\n\n#num_ftrs = model.classifier[0].in_features\n\n#num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n\nnum_ftrs = model.classifier.in_features  ## for densenet169\n\nprint(\"Number of features: \"+str(num_ftrs))\n\n#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n\n#model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n\nmodel.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n\n# Observe that all parameters are being optimized\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n\n# Decay LR by a factor of 0.1 every 7 epochs\n\n# Learning rate scheduling should be applied after optimizer’s update\n\n# e.g., you should write your code this way:\n\n# for epoch in range(100):\n\n#     train(...)\n\n#     validate(...)\n\n#     scheduler.step()\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"densenet169\")","metadata":{"id":"0xgJj2xGTarw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"41a2b62b-b0ce-4139-eb11-148add281fac","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\n\nimport numpy as np\n\n\n\n# Getting Proba distribution\n\nprint(\"\\nGetting the Probability Distribution\")\n\ntrainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n\ntestloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n\nmodel=model.eval()\n","metadata":{"id":"Pqimg_IxUQjm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e72704a-2b94-4fa1-8f9f-ddcbb19873a8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f = open(data_dir+\"/densenet169_train.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(trainloader),num_classes))\n\n      for i,data in enumerate(trainloader):\n\n          images, labels = data\n\n          sample_fname, _ = trainloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Train Accuracy = \",100*correct/total)\n\nfor i in range(len(trainloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(trainloader):\n\n  _, labels = data\n\n  sample_fname, _ = trainloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n# ================\n\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # DenseNet169 Train Probabilities\n\n# f = open(save_dir + \"/densenet169_train.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(trainloader), num_classes))\n\n\n\n#     # Loop through training data\n\n#     for i, data in enumerate(trainloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print train accuracy\n\n# print(\"Train Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(trainloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/train_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through training data to get labels\n\n# for i, data in enumerate(trainloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"D79oEFEVUM5j","colab":{"base_uri":"https://localhost:8080/"},"outputId":"910fa914-4bc1-4b7e-edfd-34b2b5049ae7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Probabilities\n\nf = open(data_dir+\"/densenet169_test.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(testloader),num_classes))\n\n      for i,data in enumerate(testloader):\n\n          images, labels = data\n\n          sample_fname, _ = testloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Test Accuracy = \",100*correct/total)\n\nfor i in range(len(testloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(testloader):\n\n  _, labels = data\n\n  sample_fname, _ = testloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n# ==================\n\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # DenseNet169 Test Probabilities\n\n# f = open(save_dir + \"/densenet169_test.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(testloader), num_classes))\n\n\n\n#     # Loop through test data\n\n#     for i, data in enumerate(testloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print test accuracy\n\n# print(\"Test Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(testloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/test_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through test data to get labels\n\n# for i, data in enumerate(testloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"hIkkc94UUNog","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2da44606-6d16-4437-eba2-007871eaa190","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Efficientnet B7 Model","metadata":{"id":"w2BmxFk25qt_"}},{"cell_type":"markdown","source":"## Efficientnet B7 Model\n\n\n\nFollowing the pattern of the previous section, this part focuses on the Efficientnet B7 model, its setup, and training.","metadata":{"id":"i-_BIBsS-Qcw"}},{"cell_type":"code","source":"model = models.efficientnet_b7(pretrained = True)\n\n#num_ftrs = model.classifier[0].in_features\n\n#num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n\n#num_ftrs = model.classifier.in_features  ## for densenet169\n\nnum_ftrs = model.classifier[1].in_features   ## for efficientnet_b7\n\nprint(\"Number of features: \"+str(num_ftrs))\n\n#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n\n#model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n\nmodel.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169, efficientnet_b7\n\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n\n# Observe that all parameters are being optimized\n\noptimizer = optim.Adam(model.parameters(), lr=0.0001)\n\n# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n\n# Decay LR by a factor of 0.1 every 7 epochs\n\n# Learning rate scheduling should be applied after optimizer’s update\n\n# e.g., you should write your code this way:\n\n# for epoch in range(100):\n\n#     train(...)\n\n#     validate(...)\n\n#     scheduler.step()\n\nstep_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n\nmodel = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"efficientnet_b7\")","metadata":{"id":"VjFMd7jC-Q4W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"46f937b6-b6ff-4996-c448-ee8c1c7a9820","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\n\nimport numpy as np\n\n# Getting Proba distribution\n\nprint(\"\\nGetting the Probability Distribution\")\n\ntrainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n\ntestloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n\nmodel=model.eval()","metadata":{"id":"nhT4agZ7-Q7n","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae0598d0-9c60-445e-bfe7-1366a4173d62","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f = open(data_dir+\"/efficientnetb7_train.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(trainloader),num_classes))\n\n      for i,data in enumerate(trainloader):\n\n          images, labels = data\n\n          sample_fname, _ = trainloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Train Accuracy = \",100*correct/total)\n\nfor i in range(len(trainloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(trainloader):\n\n  _, labels = data\n\n  sample_fname, _ = trainloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n# ======================\n\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # EfficientNetB7 Train Probabilities\n\n# f = open(save_dir + \"/efficientnetb7_train.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(trainloader), num_classes))\n\n\n\n#     # Loop through training data\n\n#     for i, data in enumerate(trainloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print train accuracy\n\n# print(\"Train Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(trainloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/train_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through training data to get labels\n\n# for i, data in enumerate(trainloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = trainloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"_x6tAo7e_tWB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b015e33f-764f-4b6e-b91c-49bd267576e4","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Test Probabilities\n\nf = open(data_dir+\"/efficientnetb7_test.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nsaving = []\n\ncorrect = 0\n\ntotal = 0\n\nwith torch.no_grad():\n\n      num = 0\n\n      temp_array = np.zeros((len(testloader),num_classes))\n\n      for i,data in enumerate(testloader):\n\n          images, labels = data\n\n          sample_fname, _ = testloader.dataset.samples[i]\n\n          labels=labels.cuda()\n\n          outputs = model(images.cuda())\n\n          _, predicted = torch.max(outputs, 1)\n\n          total += labels.size(0)\n\n          correct += (predicted == labels.cuda()).sum().item()\n\n          prob = torch.nn.functional.softmax(outputs, dim=1)\n\n          saving.append(sample_fname.split('/')[-1])\n\n          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n\n          num+=1\n\nprint(\"Test Accuracy = \",100*correct/total)\n\nfor i in range(len(testloader)):\n\n  k = temp_array[i].tolist()\n\n  k.append(saving[i])\n\n  writer.writerow(k)\n\nf.close()\n\nf = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n\nwriter = csv.writer(f)\n\nfor i,data in enumerate(testloader):\n\n  _, labels = data\n\n  sample_fname, _ = testloader.dataset.samples[i]\n\n  sample = sample_fname.split('/')[-1]\n\n  lab = labels.tolist()[0]\n\n  writer.writerow([sample,lab])\n\nf.close()\n\n\n# ====================\n\n# import csv\n\n# import numpy as np\n\n# import torch\n\n\n\n# # EfficientNetB7 Test Probabilities\n\n# f = open(save_dir + \"/efficientnetb7_test.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n# saving = []\n\n# correct = 0\n\n# total = 0\n\n\n\n# with torch.no_grad():\n\n#     num = 0\n\n#     temp_array = np.zeros((len(testloader), num_classes))\n\n\n\n#     # Loop through test data\n\n#     for i, data in enumerate(testloader):\n\n#         images, labels = data\n\n#         sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#         labels = labels.cuda()  # Move labels to GPU\n\n#         outputs = model(images.cuda())  # Get model predictions\n\n\n\n#         # Calculate predicted class and accumulate accuracy metrics\n\n#         _, predicted = torch.max(outputs, 1)\n\n#         total += labels.size(0)\n\n#         correct += (predicted == labels).sum().item()\n\n\n\n#         # Calculate probabilities using softmax and store filename and probabilities\n\n#         prob = torch.nn.functional.softmax(outputs, dim=1)\n\n#         saving.append(sample_fname.split('/')[-1])\n\n#         temp_array[num] = np.asarray(prob[0].tolist()[:num_classes])  # Store probabilities\n\n#         num += 1\n\n\n\n# # Calculate and print test accuracy\n\n# print(\"Test Accuracy = \", 100 * correct / total)\n\n\n\n# # Write predicted probabilities to CSV\n\n# for i in range(len(testloader)):\n\n#     row = temp_array[i].tolist()  # Convert probabilities to list\n\n#     row.append(saving[i])         # Append filename\n\n#     writer.writerow(row)           # Write row to CSV\n\n# f.close()  # Close the file\n\n\n\n# # Open file for writing true labels\n\n# f = open(save_dir + \"/test_labels.csv\", 'w+', newline='')\n\n# writer = csv.writer(f)\n\n\n\n# # Loop through test data to get labels\n\n# for i, data in enumerate(testloader):\n\n#     _, labels = data\n\n#     sample_fname, _ = testloader.dataset.samples[i]  # Get filename\n\n#     sample = sample_fname.split('/')[-1]  # Extract filename from path\n\n#     lab = labels.tolist()[0]              # Get label as integer\n\n#     writer.writerow([sample, lab])         # Write filename and label to CSV\n\n# f.close()  # Close the file\n","metadata":{"id":"411CiJvX_tYo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7c14f00f-1b82-4088-b776-6af7d61d8bc7","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Ensemble Learning Analysis","metadata":{"id":"T_wzB7pcMV7F"}},{"cell_type":"markdown","source":"## Imports","metadata":{"id":"F60vUT2KM9mh"}},{"cell_type":"code","source":"# !pip install scikit-plot","metadata":{"id":"xZAkFyiqq8Eb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip uninstall scipy -y","metadata":{"id":"ul3MAh6csWlV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"70e5d4b6-f557-4ff3-fe36-da768cd02105","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install numpy==1.22.4 pandas==1.5.3 scipy==1.8.1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"RlHr5ln6YwvD","outputId":"8d173398-ce98-49d3-9225-45a332894a6f","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip uninstall scikit-plot -y\n\n# !pip install scikit-plot","metadata":{"id":"sVrkFN5lsXxW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"174d425d-7ebb-4f5d-c1dd-007ecf76e509","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip cache purge","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mB0O4DAyZ9PK","outputId":"580ddcb6-240e-464b-abea-c1021e09835e","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install --force-reinstall numpy==1.22.4 scipy==1.8.1","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":981},"id":"gI9cCar2asrQ","outputId":"71e22c01-3d97-4d90-bde9-e77a8d5032dd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n\n# import numpy as np\n\n# from sklearn.metrics import *\n\n# import matplotlib.pyplot as plt\n\n# import math,os,argparse\n\n# from scikitplot.estimators import plot_feature_importances\n\n# from scikitplot.metrics import plot_confusion_matrix, plot_roc\n\n# from sklearn.metrics import roc_curve,auc\n\n# import matplotlib.pyplot as plt\n\n# from sklearn.preprocessing import label_binarize","metadata":{"id":"1yFTPabIM6cW","colab":{"base_uri":"https://localhost:8080/","height":106},"outputId":"e839e499-34a5-4bd3-ec8b-861258a9f744","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Data Loading","metadata":{"id":"_WIM3Au8NBPM"}},{"cell_type":"code","source":"parser.add_argument('--root_train', type=str, required = True, help='Directory where train csv files are stored')\n\nparser.add_argument('--train_labels', type=str, required = True, help='File path for train labels')\n\nparser.add_argument('--root_test', type=str, required = True, help='Directory where test csv files are stored')\n\nparser.add_argument('--test_labels', type=str, required = True, help='File path for test labels')\n\n    df = pd.read_csv(file,header=None)\n\n    df = pd.read_csv(file,header=None)","metadata":{"id":"lgJezXwONBi2","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper Functions","metadata":{"id":"CxHDVc1qNFtP"}},{"cell_type":"code","source":"def predicting(ensemble_prob):\n\n    prediction = np.zeros((ensemble_prob.shape[0],))\n\n    for i in range(ensemble_prob.shape[0]):\n\n        temp = ensemble_prob[i]\n\n        t = np.where(temp == np.max(temp))[0][0]\n\n        prediction[i] = t\n\n    return prediction\n\n\n\ndef getfile(filename):\n\n    root=\"./\"\n\n    file = root+filename\n\n    if '.csv' not in file:\n\n        file+='.csv'\n\n    df = pd.read_csv(file,header=None)\n\n    df = np.asarray(df)[:,:-1] #Since last column has image names\n\n    return df\n\n\n\ndef metrics(labels,predictions,classes):\n\n    print(\"Classification Report:\")\n\n    print(classification_report(labels, predictions, target_names = classes,digits = 4))\n\n    matrix = confusion_matrix(labels, predictions)\n\n    accuracy = accuracy_score(labels,predictions)\n\n    pre = precision_score(labels,predictions)\n\n    rec = recall_score(labels,predictions)\n\n    f1 = f1_score(labels,predictions)\n\n    auc = roc_auc_score(labels,predictions)\n\n    print(\"Accuracy\", accuracy)\n\n    print(\"Precision Score\", pre)\n\n    print(\"Recall Score\", rec)\n\n    print(\"F1 Score\", f1)\n\n    print(\"Roc_Auc Score\", auc)\n\n    print(\"Confusion matrix:\")\n\n    print(matrix)\n\n    print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal()/matrix.sum(axis = 1)))","metadata":{"id":"Vviab_TxNDuG","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Plot ROC Curve Function","metadata":{"id":"E7QrH7bjNMIX"}},{"cell_type":"code","source":"#ROC-AUC\n\nfrom sklearn.metrics import roc_curve,auc\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import label_binarize\n\n\n\ndef plot_roc(val_label,decision_val, caption='ROC Curve'):\n\n    num_classes=np.unique(val_label).shape[0]\n\n    classes = []\n\n    for i in range(num_classes):\n\n        classes.append(i)\n\n    plt.figure()\n\n    decision_val = label_binarize(decision_val, classes=classes)\n\n\n\n    if num_classes!=2:\n\n        # Compute ROC curve and ROC area for each class\n\n        fpr = dict()\n\n        tpr = dict()\n\n        roc_auc = dict()\n\n        for i in range(num_classes):\n\n            y_val = label_binarize(val_label, classes=classes)\n\n            fpr[i], tpr[i], _ = roc_curve(y_val[:, i], decision_val[:, i])\n\n            roc_auc[i] = auc(fpr[i], tpr[i])\n\n        for i in range(num_classes):\n\n            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n\n                                           ''.format(i+1, roc_auc[i]))\n\n    else:\n\n        fpr,tpr,_ = roc_curve(val_label,decision_val, pos_label=1)\n\n        roc_auc = auc(fpr,tpr)*100\n\n        plt.plot(fpr,tpr,label='ROC curve (AUC=%0.2f)'%roc_auc)\n\n\n\n    plt.plot([0, 1], [0, 1], 'k--')\n\n    plt.xlim([0.0, 1.0])\n\n    plt.ylim([0.0, 1.05])\n\n    plt.xlabel('False Positive Rate')\n\n    plt.ylabel('True Positive Rate')\n\n    plt.title(caption)\n\n    plt.legend(loc=\"lower right\")\n\n    plt.savefig(str(len(classes))+'RE_NAME.png',dpi=300)\n\n\n\n\n\n#plot_roc(test_labels,preds_Adjusted)\n\nplot_roc(test_labels,preds_original)","metadata":{"id":"7jQLF1IaNSKt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysis\n","metadata":{"id":"zUooQ98FNsVP"}},{"cell_type":"code","source":"# Function to calculate scores for each model\n\ndef get_scores(labels, *models_predictions):\n\n    \"\"\"\n\n    Calculates precision, recall, f1-score, and AUC for each model's predictions.\n\n\n\n    :param labels: The ground truth labels.\n\n    :param models_predictions: Variable number of arrays with model predictions.\n\n    :return: A list of weights for each set of model predictions.\n\n    \"\"\"\n\n    num_models = len(models_predictions)\n\n    # Initialize metrics array\n\n    metrics = np.zeros((2, num_models))\n\n    num_classes = len(np.unique(labels))\n\n\n\n    for i, model_preds in enumerate(models_predictions):\n\n        # Simulate a predicting function for model predictions\n\n        preds = predicting(model_preds)\n\n\n\n        # Calculate different metrics depending on the number of classes\n\n        if num_classes == 2:  # Binary classification\n\n            pre = precision_score(labels, preds)\n\n            rec = recall_score(labels, preds)\n\n            f1 = f1_score(labels, preds)\n\n            auc = roc_auc_score(labels, preds)\n\n        else:  # Multiclass classification\n\n            pre = precision_score(labels, preds, average='macro')\n\n            rec = recall_score(labels, preds, average='macro')\n\n            f1 = f1_score(labels, preds, average='macro')\n\n            auc = roc_auc_score(labels, model_preds, average='macro', multi_class='ovo')\n\n\n\n        # Update metrics array with the calculated metrics\n\n        metrics[:, i] = np.array([f1, auc])\n\n\n\n    # Output the f1 and auc scores for each model\n\n    print(\"F1 Score\", metrics[0] * 100)\n\n    print(\"ROC_AUC Score\", metrics[1] * 100)\n\n\n\n    # Calculate weights based on the metrics\n\n    weights = get_weights(np.transpose(metrics))\n\n    return weights\n\n\n\ndef get_weights(matrix):\n\n    \"\"\"\n\n    Calculates weights for each model using the tanh function on the metrics.\n\n\n\n    :param matrix: A matrix of shape (number of models, number of metrics).\n\n    :return: A list of weights for each model.\n\n    \"\"\"\n\n    weights = []\n\n    for model_metrics in matrix:\n\n        # Use tanh to compute a weighted sum of metrics for each model\n\n        weight = np.sum(np.tanh(model_metrics))\n\n        weights.append(weight)\n\n    return weights\n\n\n\n# Ensure the training root path ends with a slash\n\nroot_train = args.root_train\n\nif root_train[-1] != '/':\n\n    root_train += '/'\n\n\n\n# Ensure the testing root path ends with a slash\n\nroot_test = args.root_test\n\nif root_test[-1] != '/':\n\n    root_test += '/'\n\n\n\n# Define the filenames for the training predictions from different models\n\ntrain1 = \"densenet169_train_adam\"\n\ntrain2 = \"efficientnetb7_train_adam\"\n\ntrain3 = \"resnet152_train_adam\"\n\n\n\n# Retrieve the predictions from files for training data\n\np1_train = getfile(root_train + train1)  # Get predictions for the first model\n\np2_train = getfile(root_train + train2)  # Get predictions for the second model\n\np3_train = getfile(root_train + train3)  # Get predictions for the third model\n\n\n\n# Load the labels for the training data\n\ntrain_labels = getlabels(args.train_labels)\n\n\n\n# Define the filenames for the testing predictions from different models\n\ntest1 = \"densenet169_test_adam\"\n\ntest2 = \"efficientnetb7_test_adam\"\n\ntest3 = \"resnet152_test_adam\"\n\n\n\n# Retrieve the predictions from files for testing data\n\np1_test = getfile(root_test + test1)  # Get predictions for the first model\n\np2_test = getfile(root_test + test2)  # Get predictions for the second model\n\np3_test = getfile(root_test + test3)  # Get predictions for the third model\n\n\n\n# Load the labels for the testing data\n\ntest_labels = getlabels(args.test_labels)\n\n\n\n# Print the order of CSV files\n\nprint(\"Train CSV's Order\", train1 + \", \" + train2 + \", \" + train3)\n\nprint(\"\\n\")\n\nprint(\"Test CSV's Order\", test1 + \", \" + test2 + \", \" + test3)\n\nprint(\"\\n\")\n\n\n\n# Display header for training data metrics\n\nprint(\"Training Data Metrics\")\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(train1 + \" \" + train2 + \" \" + train3)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\n\n\n# Get weights based on training labels and predictions\n\nweights = get_scores(train_labels, p1_train, p2_train, p3_train)\n\nprint(\"\\n\")\n\n\n\n# Display header for testing data metrics\n\nprint(\"Testing Data Metrics\")\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(test1 + \" \" + test2 + \" \" + test3)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nweight = get_scores(test_labels, p1_test, p2_test, p3_test)\n\nprint(\"\\n\")\n\n\n\n# Display original weights\n\nprint(\"Original Weights\")\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(train1, weights[0])\n\nprint(train2, weights[1])\n\nprint(train3, weights[2])\n\nprint(\"\\n\")\n\n\n\n# Calculate ensemble probabilities using the original weights\n\nensemble_prob_original = weights[0] * p1_test + weights[1] * p2_test + weights[2] * p3_test\n\n\n\n# Display ensemble probabilities with original weights\n\nprint(\"Ensemble Probabilities with Original Weights\", ensemble_prob_original)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(\"\\n\")\n\n\n\n# Generate predictions using the original ensemble probabilities\n\npreds_original = predicting(ensemble_prob_original)\n\n\n\n# Display predictions with original weights\n\nprint(\"Predictions with Original Weights\", preds_original)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(\"\\n\")\n\n\n\n# Normalize the weights so they sum up to 1\n\nw0, w1, w2 = weights\n\nw0_norm = w0 / (w0 + w1 + w2)\n\nw1_norm = w1 / (w0 + w1 + w2)\n\nw2_norm = w2 / (w0 + w1 + w2)\n\n\n\n# Display normalized weights\n\nprint(\"Normalised Weights\")\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(train1, w0_norm)\n\nprint(train2, w1_norm)\n\nprint(train3, w2_norm)\n\nprint(\"\\n\")\n\n\n\n# Calculate ensemble probabilities using the normalized weights\n\nensemble_prob_Normal = w0_norm * p1_test + w1_norm * p2_test + w2_norm * p3_test\n\n\n\n# Display ensemble probabilities with normalized weights\n\nprint(\"Ensemble Probabilities with Normal Weights\", ensemble_prob_Normal)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(\"\\n\")\n\n\n\n# Generate predictions using the normalized ensemble probabilities\n\npreds_Normal = predicting(ensemble_prob_Normal)\n\n\n\n# Display predictions with normalized weights\n\nprint(\"Predictions with Normal Weights\", preds_Normal)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(\"\\n\")\n\n\n\n# Manually adjusted weights based on empirical findings\n\nw0_adj = 0.3585864 # Best Weights for some criterion\n\nw1_adj = 0.3441670 # Best Weights for some criterion\n\nw2_adj = 0.2972466 # Best Weights for some criterion\n\n\n\n# Display adjusted weights\n\nprint(\"Adjusted Weights\")\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(train1, w0_adj)\n\nprint(train2, w1_adj)\n\nprint(train3, w2_adj)\n\nprint(\"\\n\")\n\n\n\n# Calculate ensemble probabilities using the adjusted weights\n\nensemble_prob_Adjusted = w0_adj * p1_test + w1_adj * p2_test + w2_adj * p3_test\n\n\n\n# Display ensemble probabilities after updating weights\n\nprint(\"Ensemble Probabilities After Updating Weights\", ensemble_prob_Adjusted)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(\"\\n\")\n\n\n\n# Generate predictions using the adjusted ensemble probabilities\n\npreds_Adjusted = predicting(ensemble_prob_Adjusted)\n\n\n\n# Display predictions after updating weights\n\nprint(\"Predictions After Updating Weights\", preds_Adjusted)\n\nprint(\"--------------------------------------------------------------------------------------\")\n\nprint(\"\\n\")\n\n\n\n# Calculate the number of correct predictions with original weights and print accuracy\n\ncorrect_original = np.where(preds_original == test_labels)[0].shape[0]\n\ntotal = test_labels.shape[0]\n\nprint(\"\\n\")\n\nprint(\"Accuracy (Original) = \", (correct_original / total) * 100)\n\nprint(\"\\n\")\n\n\n\n# Print classification metrics for the predictions made with original weights\n\nclasses = ['Benign', 'Malignant']\n\nmetrics(test_labels, preds_original, classes)\n\n\n\n# Calculate the number of correct predictions with normalized weights and print accuracy\n\ncorrect_Normal = np.where(preds_Normal == test_labels)[0].shape[0]\n\nprint(\"\\n\")\n\nprint(\"Accuracy (Normal) = \", (correct_Normal / total) * 100)\n\nprint(\"\\n\")\n\n\n\n# Print classification metrics for the predictions made with normalized weights\n\nmetrics(test_labels, preds_Normal, classes)","metadata":{"id":"jlgPiVbaNu63","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"id":"LqKGFzBREROr","trusted":true},"outputs":[],"execution_count":null}]}